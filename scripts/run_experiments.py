"""
Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸ ML.

Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ² MLflow.
"""

import os
import pickle
import sys
import tempfile
import time
from datetime import datetime
from pathlib import Path

import matplotlib.pyplot as plt
import mlflow
import mlflow.sklearn
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from loguru import logger
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº src
PROJ_ROOT = Path(__file__).resolve().parents[1]
load_dotenv(PROJ_ROOT / ".env")
sys.path.insert(0, str(PROJ_ROOT))

from src.config import RAW_DATA_DIR, HOUSING_DATA_FILE  # noqa: E402
from src.ml_models.model_loader import MODEL_REGISTRY, create_model  # noqa: E402


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ 19 Ğ­ĞšĞ¡ĞŸĞ•Ğ Ğ˜ĞœĞ•ĞĞ¢ĞĞ’
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXPERIMENTS_CONFIG = [
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (7 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
        "name": "linear_regression",
        "params": {},
        "description": "Baseline Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ",
    },
    {
        "name": "ridge",
        "params": {"alpha": 0.1},
        "description": "Ridge ÑĞ¾ ÑĞ»Ğ°Ğ±Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹",
    },
    {"name": "ridge", "params": {"alpha": 1.0}, "description": "Ridge ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹"},
    {
        "name": "ridge",
        "params": {"alpha": 10.0},
        "description": "Ridge Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹",
    },
    {
        "name": "lasso",
        "params": {"alpha": 0.1},
        "description": "Lasso Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
    },
    {
        "name": "elastic_net",
        "params": {"alpha": 0.5, "l1_ratio": 0.5},
        "description": "Elastic Net ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹",
    },
    {
        "name": "huber",
        "params": {"epsilon": 1.35},
        "description": "Huber Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ",
    },
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ”Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸ (9 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
        "name": "decision_tree",
        "params": {"max_depth": 5},
        "description": "Ğ”ĞµÑ€ĞµĞ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ (shallow)",
    },
    {
        "name": "decision_tree",
        "params": {"max_depth": 10},
        "description": "Ğ”ĞµÑ€ĞµĞ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ (deep)",
    },
    {
        "name": "random_forest",
        "params": {"n_estimators": 100, "max_depth": 10},
        "description": "Random Forest ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹",
    },
    {
        "name": "random_forest",
        "params": {"n_estimators": 200, "max_depth": 15},
        "description": "Random Forest Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹",
    },
    {
        "name": "extra_trees",
        "params": {"n_estimators": 100, "max_depth": 10},
        "description": "Extra Trees",
    },
    {
        "name": "gradient_boosting",
        "params": {"n_estimators": 100, "learning_rate": 0.1},
        "description": "Gradient Boosting ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹",
    },
    {
        "name": "gradient_boosting",
        "params": {"n_estimators": 200, "learning_rate": 0.05},
        "description": "Gradient Boosting Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹",
    },
    {
        "name": "adaboost",
        "params": {"n_estimators": 50, "learning_rate": 1.0},
        "description": "AdaBoost",
    },
    {
        "name": "bagging",
        "params": {"n_estimators": 20},
        "description": "Bagging Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¾Ñ€",
    },
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ”Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (3 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
        "name": "svr",
        "params": {"kernel": "rbf", "C": 1.0},
        "description": "SVR Ñ RBF ÑĞ´Ñ€Ğ¾Ğ¼",
    },
    {
        "name": "knn",
        "params": {"n_neighbors": 5, "weights": "uniform"},
        "description": "KNN k=5 uniform",
    },
    {
        "name": "knn",
        "params": {"n_neighbors": 10, "weights": "distance"},
        "description": "KNN k=10 distance",
    },
]


def load_data():
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Boston Housing."""
    data_file = RAW_DATA_DIR / HOUSING_DATA_FILE

    if not data_file.exists():
        logger.error(f"Ğ¤Ğ°Ğ¹Ğ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {data_file}")
        logger.info("Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ 'dvc pull' Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
        sys.exit(1)

    df = pd.read_csv(data_file, sep=r"\s+", header=None)

    column_names = [
        "CRIM",
        "ZN",
        "INDUS",
        "CHAS",
        "NOX",
        "RM",
        "AGE",
        "DIS",
        "RAD",
        "TAX",
        "PTRATIO",
        "B",
        "LSTAT",
        "MEDV",
    ]
    df.columns = column_names

    X = df.drop("MEDV", axis=1)
    y = df["MEDV"]

    return train_test_split(X, y, test_size=0.2, random_state=42)


def get_algorithm_family(model_name: str) -> str:
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ´Ğ»Ñ Ñ‚ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."""
    linear_models = [
        "linear_regression",
        "ridge",
        "lasso",
        "elastic_net",
        "huber",
        "sgd",
    ]
    tree_models = [
        "decision_tree",
        "random_forest",
        "extra_trees",
        "gradient_boosting",
        "adaboost",
        "bagging",
    ]

    if model_name in linear_models:
        return "linear"
    elif model_name in tree_models:
        return "tree_ensemble"
    else:
        return "other"


def evaluate_model(model, X_test, y_test):
    """ĞÑ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº."""
    y_pred = model.predict(X_test)

    return {
        "r2_score": float(r2_score(y_test, y_pred)),
        "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
        "mae": float(mean_absolute_error(y_test, y_pred)),
        "mape": float(np.mean(np.abs((y_test - y_pred) / y_test)) * 100),
    }, y_pred


def create_plots(model, model_name, X_train, y_test, y_pred, temp_dir):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²."""
    plots = []

    # 1. Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ vs Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.scatter(y_test, y_pred, alpha=0.6, edgecolors="black", linewidth=0.5)
    ax.plot(
        [y_test.min(), y_test.max()],
        [y_test.min(), y_test.max()],
        "r--",
        lw=2,
        label="Ğ˜Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ",
    )
    ax.set_xlabel("Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ (MEDV)", fontsize=12)
    ax.set_ylabel("ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ", fontsize=12)
    ax.set_title(f"ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ vs Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ\n{model_name}", fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)

    scatter_path = os.path.join(temp_dir, "predictions_scatter.png")
    fig.tight_layout()
    fig.savefig(scatter_path, dpi=150)
    plt.close(fig)
    plots.append(("plots", scatter_path))

    # 2. Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ²
    residuals = y_test - y_pred
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.scatter(y_pred, residuals, alpha=0.6, edgecolors="black", linewidth=0.5)
    ax.axhline(y=0, color="r", linestyle="--", lw=2)
    ax.set_xlabel("ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ", fontsize=12)
    ax.set_ylabel("ĞÑÑ‚Ğ°Ñ‚ĞºĞ¸ (Residuals)", fontsize=12)
    ax.set_title(f"Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ²\n{model_name}", fontsize=14)
    ax.grid(True, alpha=0.3)

    residuals_path = os.path.join(temp_dir, "residuals.png")
    fig.tight_layout()
    fig.savefig(residuals_path, dpi=150)
    plt.close(fig)
    plots.append(("plots", residuals_path))

    # 3. Feature Importance (Ğ´Ğ»Ñ tree-based Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹)
    if hasattr(model, "feature_importances_"):
        fig, ax = plt.subplots(figsize=(10, 8))
        importances = pd.DataFrame(
            {"feature": X_train.columns, "importance": model.feature_importances_}
        ).sort_values("importance", ascending=True)

        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(importances)))
        ax.barh(importances["feature"], importances["importance"], color=colors)
        ax.set_xlabel("Ğ’Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°", fontsize=12)
        ax.set_title(f"Ğ’Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²\n{model_name}", fontsize=14)
        ax.grid(True, alpha=0.3, axis="x")

        importance_path = os.path.join(temp_dir, "feature_importance.png")
        fig.tight_layout()
        fig.savefig(importance_path, dpi=150)
        plt.close(fig)
        plots.append(("plots", importance_path))

    return plots


def run_single_experiment(
    config, X_train, X_test, y_train, y_test, experiment_idx, total_experiments
):
    """Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² MLflow."""

    model_name = config["name"]
    custom_params = config["params"]
    description = config.get("description", "")

    # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸ run
    param_str = "_".join([f"{k}={v}" for k, v in custom_params.items()])
    run_name = f"{model_name}_{param_str}" if param_str else model_name

    logger.info(f"\n{'â•' * 60}")
    logger.info(f"[{experiment_idx}/{total_experiments}] ğŸš€ {run_name}")
    logger.info(f"ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ: {description}")
    logger.info(f"{'â•' * 60}")

    with mlflow.start_run(run_name=run_name):
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 1. Ğ›ĞĞ“Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ ĞĞ’
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        mlflow.log_param("model_type", model_name)
        mlflow.log_param("model_description", MODEL_REGISTRY[model_name]["description"])
        mlflow.log_param("experiment_description", description)

        # ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        for param_name, param_value in custom_params.items():
            mlflow.log_param(param_name, param_value)

        # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        mlflow.log_param("train_size", len(X_train))
        mlflow.log_param("test_size", len(X_test))
        mlflow.log_param("n_features", X_train.shape[1])
        mlflow.log_param("random_state", 42)
        mlflow.log_param("test_split_ratio", 0.2)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 2. ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• ĞœĞĞ”Ğ•Ğ›Ğ˜ Ğ¡ Ğ—ĞĞœĞ•Ğ ĞĞœ Ğ’Ğ Ğ•ĞœĞ•ĞĞ˜
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        model = create_model(model_name, custom_params)

        start_train = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_train

        start_inference = time.time()
        metrics, y_pred = evaluate_model(model, X_test, y_test)
        inference_time = time.time() - start_inference

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 3. Ğ›ĞĞ“Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• ĞœĞ•Ğ¢Ğ Ğ˜Ğš
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°
        for metric_name, metric_value in metrics.items():
            mlflow.log_metric(metric_name, metric_value)

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        mlflow.log_metric("train_time_seconds", train_time)
        mlflow.log_metric("inference_time_seconds", inference_time)
        mlflow.log_metric(
            "predictions_per_second",
            len(X_test) / inference_time if inference_time > 0 else 0,
        )

        logger.info(f"  ğŸ“Š RÂ² Score:  {metrics['r2_score']:.4f}")
        logger.info(f"  ğŸ“Š RMSE:      {metrics['rmse']:.4f}")
        logger.info(f"  ğŸ“Š MAE:       {metrics['mae']:.4f}")
        logger.info(f"  ğŸ“Š MAPE:      {metrics['mape']:.2f}%")
        logger.info(f"  â±ï¸  Train:     {train_time:.3f}s")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 4. Ğ›ĞĞ“Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• ĞĞ Ğ¢Ğ•Ğ¤ĞĞšĞ¢ĞĞ’
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        with tempfile.TemporaryDirectory() as temp_dir:
            # 4.1 ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ MLflow sklearn
            # ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ: registered_model_name=None Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
            # Ğ² Model Registry (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ MLflow 3.x Ğ½Ğ° ÑĞµÑ€Ğ²ĞµÑ€Ğµ)
            try:
                mlflow.sklearn.log_model(
                    model,
                    "sklearn_model",
                    registered_model_name=None,  # ĞĞµ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² Model Registry
                )
            except Exception as e:
                logger.warning(f"  âš ï¸  ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ sklearn Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {e}")

            # 4.2 ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ² pickle Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ
            model_pkl_path = os.path.join(temp_dir, f"{model_name}.pkl")
            with open(model_pkl_path, "wb") as f:
                pickle.dump(model, f)
            mlflow.log_artifact(model_pkl_path, "model_artifacts")

            # 4.3 CSV Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸
            predictions_df = pd.DataFrame(
                {
                    "actual": y_test.values,
                    "predicted": y_pred,
                    "error": y_test.values - y_pred,
                    "abs_error": np.abs(y_test.values - y_pred),
                    "pct_error": np.abs((y_test.values - y_pred) / y_test.values) * 100,
                }
            )
            predictions_csv = os.path.join(temp_dir, "predictions.csv")
            predictions_df.to_csv(predictions_csv, index=False)
            mlflow.log_artifact(predictions_csv, "predictions")

            # 4.4 Ğ“Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸
            plots = create_plots(model, run_name, X_train, y_test, y_pred, temp_dir)
            for artifact_path, plot_path in plots:
                mlflow.log_artifact(plot_path, artifact_path)

            # 4.5 ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ° (JSON)
            import json

            config_data = {
                "model_type": model_name,
                "params": custom_params,
                "description": description,
                "data": {
                    "train_size": len(X_train),
                    "test_size": len(X_test),
                    "features": list(X_train.columns),
                },
                "metrics": metrics,
                "performance": {
                    "train_time_seconds": train_time,
                    "inference_time_seconds": inference_time,
                },
                "timestamp": datetime.now().isoformat(),
            }
            config_path = os.path.join(temp_dir, "experiment_config.json")
            with open(config_path, "w") as f:
                json.dump(config_data, f, indent=2)
            mlflow.log_artifact(config_path, "config")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 5. Ğ›ĞĞ“Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• Ğ¢Ğ•Ğ“ĞĞ’
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        mlflow.set_tag("algorithm_family", get_algorithm_family(model_name))
        mlflow.set_tag("experiment_type", "model_comparison")
        mlflow.set_tag("dataset", "boston_housing")
        mlflow.set_tag("author", "data_scientist")
        mlflow.set_tag("environment", "development")
        mlflow.set_tag(
            "mlflow.note.content",
            f"""
## Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚: {run_name}

**ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ:** {description}

**ĞœĞ¾Ğ´ĞµĞ»ÑŒ:** {MODEL_REGISTRY[model_name]["description"]}

**ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹:** {custom_params}

**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:**
- RÂ² Score: {metrics["r2_score"]:.4f}
- RMSE: {metrics["rmse"]:.4f}
- MAE: {metrics["mae"]:.4f}
- MAPE: {metrics["mape"]:.2f}%
""",
        )

        # Ğ¯Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµĞ¼ run ĞºĞ°Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¹ (Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ MLflow 3.x + ÑĞµÑ€Ğ²ĞµÑ€ 2.x)
        mlflow.end_run(status="FINISHED")

        logger.success("  âœ… Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½!")

    # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° mlflow
    return {
        "run_name": run_name,
        "model_type": model_name,
        **metrics,
        "train_time": train_time,
    }


def print_summary(results):
    """ĞŸĞµÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ´ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²."""

    df = pd.DataFrame(results)
    df = df.sort_values("r2_score", ascending=False)

    logger.info("\n")
    logger.info("=" * 80)
    logger.info("ğŸ“Š Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ¯ Ğ¡Ğ’ĞĞ”ĞšĞ Ğ­ĞšĞ¡ĞŸĞ•Ğ Ğ˜ĞœĞ•ĞĞ¢ĞĞ’")
    logger.info("=" * 80)

    logger.info("\nğŸ† Ğ¢ĞĞŸ-5 ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ ĞŸĞ RÂ² SCORE:\n")
    for i, row in df.head(5).iterrows():
        logger.info(
            f"  {row['run_name'][:40]:40} | RÂ²: {row['r2_score']:.4f} | RMSE: {row['rmse']:.4f}"
        )

    logger.info("\nğŸ“ˆ Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ ĞŸĞ Ğ¡Ğ•ĞœĞ•Ğ™Ğ¡Ğ¢Ğ’ĞĞœ ĞĞ›Ğ“ĞĞ Ğ˜Ğ¢ĞœĞĞ’:\n")

    # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ°Ğ¼
    df["family"] = df["model_type"].apply(get_algorithm_family)

    for family in df["family"].unique():
        family_df = df[df["family"] == family]
        best = family_df.loc[family_df["r2_score"].idxmax()]
        logger.info(
            f"  {family.upper():15} | Best RÂ²: {best['r2_score']:.4f} | Model: {best['model_type']}"
        )

    logger.info("\n" + "=" * 80)
    logger.info(f"âœ… Ğ’ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ {len(results)} ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²")
    logger.info("=" * 80)

    return df


def main():
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²."""

    logger.info("\n")
    logger.info("ğŸ”¬" * 30)
    logger.info("  Ğ—ĞĞŸĞ£Ğ¡Ğš ĞœĞĞ¡Ğ¡ĞĞ’Ğ«Ğ¥ Ğ­ĞšĞ¡ĞŸĞ•Ğ Ğ˜ĞœĞ•ĞĞ¢ĞĞ’ ML")
    logger.info("ğŸ”¬" * 30)
    logger.info("\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° MLflow
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://localhost:5000")
    logger.info(f"ğŸ”— MLflow Tracking URI: {mlflow_uri}")

    mlflow.set_tracking_uri(mlflow_uri)

    experiment_name = "boston_housing_model_comparison"
    mlflow.set_experiment(experiment_name)
    logger.info(f"ğŸ“ Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚: {experiment_name}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    logger.info("\nğŸ“‚ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Boston Housing...")
    X_train, X_test, y_train, y_test = load_data()
    logger.info(f"  Train: {len(X_train)} samples")
    logger.info(f"  Test:  {len(X_test)} samples")
    logger.info(f"  Features: {list(X_train.columns)}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²ÑĞµÑ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    logger.info(f"\nğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº {len(EXPERIMENTS_CONFIG)} ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²...\n")

    results = []
    total = len(EXPERIMENTS_CONFIG)

    for i, config in enumerate(EXPERIMENTS_CONFIG, 1):
        try:
            result = run_single_experiment(
                config, X_train, X_test, y_train, y_test, i, total
            )
            results.append(result)
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğµ {config['name']}: {e}")
            continue

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ¡Ğ²Ğ¾Ğ´ĞºĞ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    results_df = print_summary(results)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² CSV
    results_path = (
        Path(__file__).parent.parent / "data" / "experiments" / "results_summary.csv"
    )
    results_path.parent.mkdir(parents=True, exist_ok=True)
    results_df.to_csv(results_path, index=False)
    logger.info(f"\nğŸ’¾ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {results_path}")

    logger.info(f"\nğŸŒ ĞÑ‚ĞºÑ€Ğ¾Ğ¹Ñ‚Ğµ MLflow UI: {mlflow_uri}")
    logger.info("   Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²\n")


if __name__ == "__main__":
    main()
