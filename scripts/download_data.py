"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ Boston Housing –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.

–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
1. Statlib (CMU) - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞
2. –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —á–µ—Ä–µ–∑ scikit-learn (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
"""

import sys
from pathlib import Path
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError

from loguru import logger

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
PROJ_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJ_ROOT))

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –∏–º–ø–æ—Ä—Ç–æ–≤
DATA_DIR = PROJ_ROOT / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
HOUSING_DATA_FILE = "housing.csv"

# URL –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
DATA_SOURCES = [
    {
        "name": "Statlib (CMU)",
        "url": "http://lib.stat.cmu.edu/datasets/boston",
        "type": "statlib",
    },
    {
        "name": "GitHub Mirror (selva86)",
        "url": "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv",
        "type": "csv",
    },
]


def download_from_statlib(url: str) -> str | None:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ Statlib.

    Statlib —Ñ–æ—Ä–º–∞—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –¥–∞–Ω–Ω—ã–µ –≤ –æ—Å–æ–±–æ–º —Ñ–æ—Ä–º–∞—Ç–µ:
    - –ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å –∑–∞–Ω–∏–º–∞–µ—Ç 2 —Å—Ç—Ä–æ–∫–∏ (11 –∑–Ω–∞—á–µ–Ω–∏–π + 3 –∑–Ω–∞—á–µ–Ω–∏—è = 14 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    - –ù—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –ø–æ–ø–∞—Ä–Ω–æ
    """
    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Statlib: {url}")

    try:
        request = Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urlopen(request, timeout=30) as response:
            content = response.read().decode("latin-1")

        # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö (–ø–æ—Å–ª–µ –æ–ø–∏—Å–∞–Ω–∏—è)
        lines = content.strip().split("\n")
        raw_data_lines = []
        in_data = False

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –æ–ø–∏—Å–∞–Ω–∏—è, –∏—â–µ–º —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            parts = line.split()
            if parts and len(parts) >= 2:
                try:
                    float(parts[0])
                    in_data = True
                except ValueError:
                    continue

            if in_data:
                raw_data_lines.append(line)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–æ–ø–∞—Ä–Ω–æ (–∫–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å = 2 —Å—Ç—Ä–æ–∫–∏)
        if raw_data_lines:
            combined_lines = []
            for i in range(0, len(raw_data_lines), 2):
                if i + 1 < len(raw_data_lines):
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–≤–µ —Å—Ç—Ä–æ–∫–∏ –≤ –æ–¥–Ω—É
                    combined = raw_data_lines[i] + " " + raw_data_lines[i + 1]
                    combined_lines.append(combined)
            return "\n".join(combined_lines)
        return None

    except (URLError, HTTPError, TimeoutError) as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ Statlib: {e}")
        return None


def download_csv(url: str) -> str | None:
    """–ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ –Ω–∞–ø—Ä—è–º—É—é."""
    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ CSV: {url}")

    try:
        request = Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urlopen(request, timeout=30) as response:
            content = response.read().decode("utf-8")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º CSV —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–ø—Ä–æ–±–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å)
        lines = content.strip().split("\n")
        if lines and "," in lines[0]:
            # –≠—Ç–æ CSV —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
            data_lines = []
            for line in lines[1:]:
                # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—ã–µ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
                parts = line.split(",")
                data_lines.append(" ".join(parts))
            return "\n".join(data_lines)
        return content

    except (URLError, HTTPError, TimeoutError) as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV: {e}")
        return None


def load_from_sklearn() -> str | None:
    """
    –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ scikit-learn.

    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö sklearn –¥–∞—Ç–∞—Å–µ—Ç —É–¥–∞–ª—ë–Ω –∏–∑-–∑–∞
    —ç—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–æ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö.
    """
    try:
        logger.info("üì• –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ scikit-learn...")
        from sklearn.datasets import load_boston

        data = load_boston()
        lines = []
        for i in range(len(data.data)):
            row = list(data.data[i]) + [data.target[i]]
            lines.append(" ".join(map(str, row)))
        return "\n".join(lines)

    except ImportError:
        logger.warning("‚ö†Ô∏è scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ load_boston –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return None
    except AttributeError:
        logger.warning("‚ö†Ô∏è load_boston —É–¥–∞–ª—ë–Ω –∏–∑ scikit-learn (–≤–µ—Ä—Å–∏—è >= 1.2)")
        return None


def download_data() -> bool:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö.

    –ü—Ä–æ–±—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏.
    Returns:
        True –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
    """
    logger.info("üè† –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ Boston Housing\n")

    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
    output_path = RAW_DATA_DIR / HOUSING_DATA_FILE

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
    if output_path.exists():
        logger.info(f"‚úÖ –§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {output_path}")
        logger.info("   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏")
        return True

    data_content = None

    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    for source in DATA_SOURCES:
        logger.info(f"\nüîó –ò—Å—Ç–æ—á–Ω–∏–∫: {source['name']}")

        if source["type"] == "statlib":
            data_content = download_from_statlib(source["url"])
        elif source["type"] == "csv":
            data_content = download_csv(source["url"])

        if data_content:
            logger.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {source['name']}")
            break

    # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —á–µ—Ä–µ–∑ sklearn
    if not data_content:
        data_content = load_from_sklearn()

    if not data_content:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞")
        return False

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    with open(output_path, "w") as f:
        f.write(data_content)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    lines = data_content.strip().split("\n")
    logger.success(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
    logger.info(f"   –ó–∞–ø–∏—Å–µ–π: {len(lines)}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
    logger.info("\nüìã –ü–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏:")
    for line in lines[:3]:
        logger.info(f"   {line[:80]}...")

    return True


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞."""
    import argparse

    parser = argparse.ArgumentParser(
        description="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ Boston Housing –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"
    )
    parser.add_argument(
        "--force", "-f", action="store_true", help="–ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª"
    )
    args = parser.parse_args()

    output_path = RAW_DATA_DIR / HOUSING_DATA_FILE

    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω --force
    if args.force and output_path.exists():
        output_path.unlink()
        logger.info(f"üóëÔ∏è –£–¥–∞–ª—ë–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª: {output_path}")

    success = download_data()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
