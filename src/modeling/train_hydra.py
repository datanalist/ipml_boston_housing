"""
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Hydra –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—É—Å–∫ (Random Forest –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    uv run python src/modeling/train_hydra.py

    # –°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏
    uv run python src/modeling/train_hydra.py model=gradient_boosting

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    uv run python src/modeling/train_hydra.py model=random_forest model.n_estimators=500

    # –ì–æ—Ç–æ–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
    uv run python src/modeling/train_hydra.py +experiment=tuned

    # Multirun (–Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π)
    uv run python src/modeling/train_hydra.py --multirun model=ridge,lasso,elastic_net
"""

import pickle
import sys
from pathlib import Path

import hydra
import numpy as np
import pandas as pd
from dvclive import Live
from loguru import logger
from omegaconf import DictConfig, OmegaConf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score, train_test_split

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).resolve().parents[2]))

from src.config import MODELS_DIR, RAW_DATA_DIR, HOUSING_DATA_FILE
from src.ml_models.model_loader import create_model as create_sklearn_model
from src.schemas import ExperimentConfig


def load_data(data_config: dict) -> tuple[pd.DataFrame, pd.Series]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Boston Housing —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    raw_path = data_config.get("raw_path", "data/raw/housing.csv")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    data_path = Path(raw_path)
    if not data_path.is_absolute():
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
        project_root = Path(__file__).resolve().parents[2]
        data_path = project_root / raw_path

    if not data_path.exists():
        # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å
        data_path = RAW_DATA_DIR / HOUSING_DATA_FILE

    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}")

    # –ß—Ç–µ–Ω–∏–µ CSV
    separator = data_config.get("separator", r"\s+")
    header = data_config.get("header", None)
    df = pd.read_csv(data_path, sep=separator, header=header)

    # –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
    column_names = [
        "CRIM",
        "ZN",
        "INDUS",
        "CHAS",
        "NOX",
        "RM",
        "AGE",
        "DIS",
        "RAD",
        "TAX",
        "PTRATIO",
        "B",
        "LSTAT",
        "MEDV",
    ]
    df.columns = column_names

    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    target_column = data_config.get("target_column", "MEDV")
    X = df.drop(target_column, axis=1)
    y = df[target_column]

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(X.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    return X, y


def validate_config(cfg: DictConfig) -> ExperimentConfig:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Pydantic."""
    try:
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º OmegaConf –≤ dict
        config_dict = OmegaConf.to_container(cfg, resolve=True)

        # –°–æ–∑–¥–∞—ë–º ExperimentConfig –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        exp_config = ExperimentConfig(
            model=config_dict.get("model", {}),
            data=config_dict.get("data", {}),
            training=config_dict.get("training", {}),
            name=config_dict.get("name", "default"),
            description=config_dict.get("description", ""),
            tags=config_dict.get("tags", []),
        )

        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        model_config, data_config, training_config = exp_config.validate_all()

        logger.success(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω–∞: model={model_config.name}")
        return exp_config

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        raise


def evaluate_model(model, X_test: pd.DataFrame, y_test: pd.Series) -> dict[str, float]:
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ä–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫."""
    y_pred = model.predict(X_test)

    metrics = {
        "r2_score": float(r2_score(y_test, y_pred)),
        "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
        "mae": float(mean_absolute_error(y_test, y_pred)),
        "mape": float(np.mean(np.abs((y_test - y_pred) / y_test)) * 100),
    }

    return metrics


@hydra.main(version_base=None, config_path="../../conf", config_name="config")
def main(cfg: DictConfig) -> float:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å Hydra.

    Args:
        cfg: Hydra –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

    Returns:
        R¬≤ score –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    # –õ–æ–≥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    logger.info("=" * 60)
    logger.info("HYDRA CONFIGURATION")
    logger.info("=" * 60)
    logger.info(f"\n{OmegaConf.to_yaml(cfg)}")

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Pydantic
    exp_config = validate_config(cfg)
    model_config = exp_config.get_validated_model_config()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    model_name = model_config.name
    model_params = model_config.get_params()
    data_config = OmegaConf.to_container(cfg.data, resolve=True)
    training_dict = OmegaConf.to_container(cfg.training, resolve=True)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    test_size = training_dict.get("test_size", 0.2)
    random_state = training_dict.get("random_state", 42)
    use_cv = training_dict.get("cross_validation", False)
    cv_folds = training_dict.get("cv_folds", 5)
    use_dvclive = training_dict.get("use_dvclive", True)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y = load_data(data_config)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    logger.info(f"Train: {len(X_train)}, Test: {len(X_test)}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ model_loader
    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
    logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {model_params}")

    model = create_sklearn_model(model_name, custom_params=model_params)

    # DVCLive –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if use_dvclive:
        live = Live(save_dvc_exp=training_dict.get("save_dvc_exp", True))
    else:
        live = None

    try:
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if live:
            live.log_param("model_name", model_name)
            for key, value in model_params.items():
                live.log_param(f"model.{key}", value)
            live.log_param("test_size", test_size)
            live.log_param("random_state", random_state)
            live.log_param("n_samples", len(X))
            live.log_param("n_features", len(X.columns))

        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
        if use_cv:
            logger.info(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: {cv_folds} —Ñ–æ–ª–¥–æ–≤")
            cv_scores = cross_val_score(
                model, X_train, y_train, cv=cv_folds, scoring="r2"
            )
            logger.info(f"CV R¬≤ scores: {cv_scores}")
            logger.info(f"CV R¬≤ mean: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

            if live:
                live.log_metric("cv_r2_mean", float(cv_scores.mean()))
                live.log_metric("cv_r2_std", float(cv_scores.std()))

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        model.fit(X_train, y_train)
        logger.success("–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        metrics = evaluate_model(model, X_test, y_test)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        for metric_name, metric_value in metrics.items():
            logger.info(f"{metric_name}: {metric_value:.4f}")
            if live:
                live.log_metric(metric_name, metric_value)

        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
        if hasattr(model, "feature_importances_"):
            feature_importance = pd.DataFrame(
                {"feature": X.columns, "importance": model.feature_importances_}
            ).sort_values("importance", ascending=False)

            logger.info("\nüìä –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for _, row in feature_importance.head(5).iterrows():
                logger.info(f"  {row['feature']}: {row['importance']:.4f}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        model_filename = f"{model_name}_hydra.pkl"
        model_path = MODELS_DIR / model_filename

        with open(model_path, "wb") as f:
            pickle.dump(model, f)

        logger.success(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

        if live:
            live.log_artifact(str(model_path), type="model", name=model_name)

        # –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        logger.info("\n" + "=" * 50)
        logger.info("üìà –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
        logger.info(f"  R¬≤ Score:  {metrics['r2_score']:.4f}")
        logger.info(f"  RMSE:      {metrics['rmse']:.4f}")
        logger.info(f"  MAE:       {metrics['mae']:.4f}")
        logger.info(f"  MAPE:      {metrics['mape']:.2f}%")
        logger.info("=" * 50)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º R¬≤ –¥–ª—è Hydra multirun optimization
        return metrics["r2_score"]

    finally:
        if live:
            live.end()


if __name__ == "__main__":
    main()
