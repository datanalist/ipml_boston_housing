"""
–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –≤ –ë–æ—Å—Ç–æ–Ω–µ.
–ú–µ—Ç—Ä–∏–∫–∏ –≤—ã–≤–æ–¥—è—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —á–µ—Ä–µ–∑ DVCLive.
"""

import pickle
from pathlib import Path

import click
import numpy as np
import pandas as pd
from dvclive import Live
from loguru import logger
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

import sys

sys.path.insert(0, str(Path(__file__).resolve().parents[2]))

from src.config import MODELS_DIR, RAW_DATA_DIR, HOUSING_DATA_FILE


def load_data(data_path: Path) -> tuple[pd.DataFrame, pd.Series]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Boston Housing."""
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}")

    # –ß—Ç–µ–Ω–∏–µ CSV –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø—Ä–æ–±–µ–ª–∞–º–∏)
    df = pd.read_csv(data_path, sep=r"\s+", header=None)

    # –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
    column_names = [
        "CRIM",
        "ZN",
        "INDUS",
        "CHAS",
        "NOX",
        "RM",
        "AGE",
        "DIS",
        "RAD",
        "TAX",
        "PTRATIO",
        "B",
        "LSTAT",
        "MEDV",
    ]
    df.columns = column_names

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    X = df.drop("MEDV", axis=1)
    y = df["MEDV"]

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(X.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    return X, y


def train_random_forest(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    n_estimators: int = 100,
    max_depth: int | None = None,
    min_samples_split: int = 2,
    min_samples_leaf: int = 1,
    random_state: int = 42,
) -> RandomForestRegressor:
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest."""
    logger.info("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest...")

    model = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=random_state,
        n_jobs=-1,
    )

    model.fit(X_train, y_train)
    logger.success("–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")

    return model


def evaluate_model(
    model: RandomForestRegressor, X_test: pd.DataFrame, y_test: pd.Series
) -> dict[str, float]:
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ä–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫."""
    y_pred = model.predict(X_test)

    metrics = {
        "r2_score": r2_score(y_test, y_pred),
        "rmse": np.sqrt(mean_squared_error(y_test, y_pred)),
        "mae": mean_absolute_error(y_test, y_pred),
        "mape": np.mean(np.abs((y_test - y_pred) / y_test)) * 100,
    }

    return metrics


@click.command()
@click.option(
    "--n-estimators", "-n", default=100, type=int, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –ª–µ—Å—É"
)
@click.option(
    "--max-depth",
    "-d",
    default=10,
    type=int,
    help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ (0 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)",
)
@click.option(
    "--min-samples-split",
    "-s",
    default=5,
    type=int,
    help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —É–∑–ª–∞",
)
@click.option(
    "--min-samples-leaf",
    "-l",
    default=2,
    type=int,
    help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –ª–∏—Å—Ç–µ",
)
@click.option(
    "--test-size",
    "-t",
    default=0.2,
    type=float,
    help="–î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (0.0-1.0)",
)
@click.option(
    "--random-state", "-r", default=42, type=int, help="Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏"
)
@click.option(
    "--data-path",
    default=None,
    type=click.Path(exists=False),
    help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞–Ω–Ω—ã—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/raw/housing.csv)",
)
def main(
    n_estimators: int,
    max_depth: int,
    min_samples_split: int,
    min_samples_leaf: int,
    test_size: float,
    random_state: int,
    data_path: str | None,
):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest –Ω–∞ –¥–∞–Ω–Ω—ã—Ö Boston Housing."""

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ max_depth: 0 –æ–∑–Ω–∞—á–∞–µ—Ç None (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
    actual_max_depth = None if max_depth == 0 else max_depth

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    params = {
        "n_estimators": n_estimators,
        "max_depth": actual_max_depth,
        "min_samples_split": min_samples_split,
        "min_samples_leaf": min_samples_leaf,
        "random_state": random_state,
        "test_size": test_size,
    }

    # –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    if data_path:
        data_file = Path(data_path)
    else:
        data_file = RAW_DATA_DIR / HOUSING_DATA_FILE

    if not data_file.exists():
        logger.error(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_file}")
        logger.info("–í—ã–ø–æ–ª–Ω–∏—Ç–µ 'dvc pull' –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ MinIO")
        raise click.Abort()

    # DVCLive –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    with Live(save_dvc_exp=True) as live:
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        for param_name, param_value in params.items():
            live.log_param(param_name, param_value)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = load_data(data_file)
        live.log_param("n_samples", len(X))
        live.log_param("n_features", len(X.columns))

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=params["test_size"], random_state=params["random_state"]
        )

        live.log_param("train_size", len(X_train))
        live.log_param("test_size_actual", len(X_test))

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = train_random_forest(
            X_train,
            y_train,
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"],
            min_samples_split=params["min_samples_split"],
            min_samples_leaf=params["min_samples_leaf"],
            random_state=params["random_state"],
        )

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        metrics = evaluate_model(model, X_test, y_test)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —á–µ—Ä–µ–∑ DVCLive
        for metric_name, metric_value in metrics.items():
            live.log_metric(metric_name, metric_value)
            logger.info(f"{metric_name}: {metric_value:.4f}")

        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_importance = pd.DataFrame(
            {"feature": X.columns, "importance": model.feature_importances_}
        ).sort_values("importance", ascending=False)

        logger.info("\nüìä –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        for _, row in feature_importance.head(5).iterrows():
            logger.info(f"  {row['feature']}: {row['importance']:.4f}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        model_path = MODELS_DIR / "random_forest.pkl"

        with open(model_path, "wb") as f:
            pickle.dump(model, f)

        logger.success(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

        # –õ–æ–≥–∏—Ä—É–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –º–æ–¥–µ–ª–∏
        live.log_artifact(str(model_path), type="model", name="random_forest")

        # –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        logger.info("\n" + "=" * 50)
        logger.info("üìà –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
        logger.info(f"  R¬≤ Score:  {metrics['r2_score']:.4f}")
        logger.info(f"  RMSE:      {metrics['rmse']:.4f}")
        logger.info(f"  MAE:       {metrics['mae']:.4f}")
        logger.info(f"  MAPE:      {metrics['mape']:.2f}%")
        logger.info("=" * 50)


if __name__ == "__main__":
    main()
