"""
DAG: Boston Housing Cached Pipeline
====================================
ÐŸÐ°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ñ‡ÐµÑ€ÐµÐ· MinIO.

ÐžÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸:
- ShortCircuitOperator Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ° Ð·Ð°Ð´Ð°Ñ‡ ÐµÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² ÐºÑÑˆÐµ
- ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ…ÑÑˆÐ° Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
- ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² ÐºÑÑˆ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
"""

import sys
from datetime import datetime, timedelta
from pathlib import Path

from airflow.decorators import dag, task
from airflow.operators.python import ShortCircuitOperator

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿ÑƒÑ‚Ð¸ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
sys.path.insert(0, "/opt/airflow")
sys.path.insert(0, "/opt/airflow/src")
sys.path.insert(0, "/opt/airflow/plugins")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ÐšÐžÐÐ¤Ð˜Ð“Ð£Ð ÐÐ¦Ð˜Ð¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

default_args = {
    "owner": "boston_housing",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=2),
}

DATA_DIR = Path("/opt/airflow/data")
RAW_DATA_DIR = DATA_DIR / "raw"
MODELS_DIR = DATA_DIR / "models"
HOUSING_DATA_FILE = "housing.csv"

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
MODEL_PARAMS = {
    "n_estimators": 100,
    "max_depth": 10,
    "min_samples_split": 5,
    "min_samples_leaf": 2,
    "random_state": 42,
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DAG DEFINITION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@dag(
    dag_id="boston_housing_cached",
    default_args=default_args,
    description="ML Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð² MinIO",
    schedule=None,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["ml", "boston_housing", "caching"],
)
def boston_housing_cached_dag():
    """
    DAG Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² MinIO.

    Ð•ÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ Ñ‚Ð°ÐºÐ¸Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸ ÑƒÐ¶Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð° Ð½Ð° Ñ‚ÐµÑ… Ð¶Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… -
    Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ.
    """

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task
    def download_data() -> str:
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Boston Housing."""
        from urllib.request import Request, urlopen

        from loguru import logger

        RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
        output_path = RAW_DATA_DIR / HOUSING_DATA_FILE

        if output_path.exists():
            logger.info(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‚: {output_path}")
            return str(output_path)

        url = "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"
        logger.info(f"ðŸ“¥ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· {url}")

        request = Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urlopen(request, timeout=30) as response:
            content = response.read().decode("utf-8")

        lines = content.strip().split("\n")
        if lines and "," in lines[0]:
            data_lines = [" ".join(line.split(",")) for line in lines[1:]]
            content = "\n".join(data_lines)

        with open(output_path, "w") as f:
            f.write(content)

        logger.success(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: {output_path}")
        return str(output_path)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÑÑˆÐ°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def check_cache_exists(data_path: str, **kwargs) -> bool:
        """
        ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² ÐºÑÑˆÐµ.

        Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ñ ShortCircuitOperator:
        - True: Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ (Ð½ÐµÑ‚ ÐºÑÑˆÐ°, Ð½ÑƒÐ¶Ð½Ð¾ Ð¾Ð±ÑƒÑ‡Ð°Ñ‚ÑŒ)
        - False: Ð¿Ñ€Ð¾Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ downstream Ð·Ð°Ð´Ð°Ñ‡Ð¸ (ÐºÑÑˆ Ð½Ð°Ð¹Ð´ÐµÐ½)
        """
        from minio_cache import MinIOCache
        from loguru import logger

        logger.info("ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÑÑˆÐ° Ð² MinIO...")

        try:
            cache = MinIOCache(bucket_name="airflow-cache")
            prefix = "models/random_forest_cached"
            exists, cache_key = cache.check_cache(prefix, MODEL_PARAMS, data_path)

            if exists:
                logger.info(f"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð² ÐºÑÑˆÐµ: {cache_key}")
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÐºÐ»ÑŽÑ‡ ÐºÑÑˆÐ° Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð·Ð¶Ðµ
                kwargs["ti"].xcom_push(key="cache_key", value=cache_key)
                return False  # ÐŸÑ€Ð¾Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
            else:
                logger.info("âŒ ÐšÑÑˆ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½, Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ")
                kwargs["ti"].xcom_push(key="cache_key", value=cache_key)
                return True  # ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ

        except Exception as e:
            logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ ÐºÑÑˆÐ°: {e}")
            return True  # ÐŸÑ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ - Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK: ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task
    def train_model(data_path: str) -> dict:
        """ÐžÐ±ÑƒÑ‡Ð°ÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Random Forest."""
        import pickle

        import numpy as np
        import pandas as pd
        from loguru import logger
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
        from sklearn.model_selection import train_test_split

        logger.info("ðŸš€ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Random Forest")

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        df = pd.read_csv(data_path, sep=r"\s+", header=None)
        column_names = [
            "CRIM",
            "ZN",
            "INDUS",
            "CHAS",
            "NOX",
            "RM",
            "AGE",
            "DIS",
            "RAD",
            "TAX",
            "PTRATIO",
            "B",
            "LSTAT",
            "MEDV",
        ]
        df.columns = column_names

        X = df.drop("MEDV", axis=1)
        y = df["MEDV"]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ
        model = RandomForestRegressor(**MODEL_PARAMS, n_jobs=-1)
        model.fit(X_train, y_train)

        # ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸
        y_pred = model.predict(X_test)
        metrics = {
            "r2_score": float(r2_score(y_test, y_pred)),
            "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
            "mae": float(mean_absolute_error(y_test, y_pred)),
        }

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        model_path = MODELS_DIR / "random_forest_cached.pkl"
        with open(model_path, "wb") as f:
            pickle.dump(model, f)

        logger.success(f"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð°: RÂ²={metrics['r2_score']:.4f}")

        return {
            "model_path": str(model_path),
            "data_path": data_path,
            "params": MODEL_PARAMS,
            "metrics": metrics,
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² ÐºÑÑˆ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task
    def save_to_cache(train_result: dict) -> dict:
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² ÐºÑÑˆ MinIO."""
        from minio_cache import save_model_to_cache
        from loguru import logger

        logger.info("ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² ÐºÑÑˆ MinIO")

        try:
            model_uri = save_model_to_cache(
                model_path=train_result["model_path"],
                model_name="random_forest_cached",
                params=train_result["params"],
                data_path=train_result["data_path"],
                metrics=train_result["metrics"],
                bucket_name="airflow-cache",
            )

            logger.success(f"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° Ð² ÐºÑÑˆ: {model_uri}")

            return {
                "status": "saved",
                "cache_uri": model_uri,
                "metrics": train_result["metrics"],
            }

        except Exception as e:
            logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð² ÐºÑÑˆ: {e}")
            return {
                "status": "error",
                "error": str(e),
                "metrics": train_result["metrics"],
            }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task(trigger_rule="none_failed")
    def use_cached_model(data_path: str) -> dict:
        """
        Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð· ÐºÑÑˆÐ° (ÐµÑÐ»Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð±Ñ‹Ð»Ð¾ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾).
        trigger_rule="none_failed" Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒÑÑ Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸
        upstream Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð±Ñ‹Ð»Ð¸ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ñ‹.
        """
        from minio_cache import MinIOCache
        from loguru import logger

        logger.info("ðŸ“¦ ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸")

        try:
            cache = MinIOCache(bucket_name="airflow-cache")
            prefix = "models/random_forest_cached"
            exists, cache_key = cache.check_cache(prefix, MODEL_PARAMS, data_path)

            if exists:
                # Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
                metadata = cache.get_json(f"{cache_key}_metadata.json")

                logger.success("âœ… Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ")
                logger.info(f"   RÂ² Score: {metadata['metrics']['r2_score']:.4f}")

                return {
                    "status": "from_cache",
                    "cache_key": cache_key,
                    "metrics": metadata["metrics"],
                }
            else:
                logger.info("âŒ ÐšÑÑˆ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
                return {"status": "no_cache"}

        except Exception as e:
            logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ ÐºÑÑˆÐ°: {e}")
            return {"status": "error", "error": str(e)}

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task(trigger_rule="none_failed_min_one_success")
    def generate_summary(
        data_path: str,
        cache_result: dict = None,
        train_save_result: dict = None,
    ) -> dict:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚."""
        from loguru import logger

        logger.info("ðŸ“ Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚Ð°")

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°
        if train_save_result and train_save_result.get("status") == "saved":
            source = "trained"
            metrics = train_save_result.get("metrics", {})
        elif cache_result and cache_result.get("status") == "from_cache":
            source = "cached"
            metrics = cache_result.get("metrics", {})
        else:
            source = "unknown"
            metrics = {}

        summary = {
            "source": source,
            "data_path": data_path,
            "params": MODEL_PARAMS,
            "metrics": metrics,
        }

        if source == "cached":
            logger.success("âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½ Ð¸Ð· ÐºÑÑˆÐ° (Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾)")
        elif source == "trained":
            logger.success("âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð° Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° Ð² ÐºÑÑˆ")
        else:
            logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°")

        if metrics:
            logger.info(f"ðŸ“Š RÂ² Score: {metrics.get('r2_score', 'N/A')}")

        return summary

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ÐŸÐžÐ¡Ð¢Ð ÐžÐ•ÐÐ˜Ð• DAG
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # 1. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
    data_path = download_data()

    # 2. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÑÑˆÐ° (ShortCircuitOperator)
    cache_check = ShortCircuitOperator(
        task_id="check_cache",
        python_callable=check_cache_exists,
        op_kwargs={"data_path": data_path},
        provide_context=True,
    )

    # 3. ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ ÐµÑÐ»Ð¸ ÐºÑÑˆ Ð½Ð°Ð¹Ð´ÐµÐ½)
    train_result = train_model(data_path)

    # 4. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² ÐºÑÑˆ
    save_result = save_to_cache(train_result)

    # 5. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð²ÑÐµÐ³Ð´Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ)
    cached_result = use_cached_model(data_path)

    # 6. Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚
    summary = generate_summary(
        data_path=data_path,
        cache_result=cached_result,
        train_save_result=save_result,
    )

    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹
    data_path >> cache_check >> train_result >> save_result >> summary
    data_path >> cached_result >> summary


# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ DAG
boston_housing_cached_dag()
