"""
DAG: Boston Housing Experiments Pipeline
========================================
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ 19 –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
1. download_data - –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
2. validate_data - –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
3. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (3 –≥—Ä—É–ø–ø—ã):
   - Linear Models (7 –º–æ–¥–µ–ª–µ–π)
   - Tree Models (9 –º–æ–¥–µ–ª–µ–π)
   - Other Models (3 –º–æ–¥–µ–ª–∏)
4. aggregate_results - –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
5. generate_report - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞
"""

import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

from airflow.decorators import dag, task

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, "/opt/airflow")
sys.path.insert(0, "/opt/airflow/src")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

default_args = {
    "owner": "boston_housing",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=2),
}

# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
DATA_DIR = Path("/opt/airflow/data")
RAW_DATA_DIR = DATA_DIR / "raw"
MODELS_DIR = DATA_DIR / "models"
EXPERIMENTS_DIR = DATA_DIR / "experiments"
HOUSING_DATA_FILE = "housing.csv"

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø 19 –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ (7 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤)
LINEAR_MODELS = [
    {
        "name": "linear_regression",
        "params": {},
        "description": "Baseline –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è",
    },
    {"name": "ridge", "params": {"alpha": 0.1}, "description": "Ridge Œ±=0.1"},
    {"name": "ridge", "params": {"alpha": 1.0}, "description": "Ridge Œ±=1.0"},
    {"name": "ridge", "params": {"alpha": 10.0}, "description": "Ridge Œ±=10.0"},
    {"name": "lasso", "params": {"alpha": 0.1}, "description": "Lasso Œ±=0.1"},
    {
        "name": "elastic_net",
        "params": {"alpha": 0.5, "l1_ratio": 0.5},
        "description": "ElasticNet",
    },
    {"name": "huber", "params": {"epsilon": 1.35}, "description": "Huber Regressor"},
]

# –î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∞–Ω—Å–∞–º–±–ª–∏ (9 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤)
TREE_MODELS = [
    {
        "name": "decision_tree",
        "params": {"max_depth": 5},
        "description": "Decision Tree d=5",
    },
    {
        "name": "decision_tree",
        "params": {"max_depth": 10},
        "description": "Decision Tree d=10",
    },
    {
        "name": "random_forest",
        "params": {"n_estimators": 100, "max_depth": 10},
        "description": "RF n=100",
    },
    {
        "name": "random_forest",
        "params": {"n_estimators": 200, "max_depth": 15},
        "description": "RF n=200",
    },
    {
        "name": "extra_trees",
        "params": {"n_estimators": 100, "max_depth": 10},
        "description": "ExtraTrees",
    },
    {
        "name": "gradient_boosting",
        "params": {"n_estimators": 100, "learning_rate": 0.1},
        "description": "GBM lr=0.1",
    },
    {
        "name": "gradient_boosting",
        "params": {"n_estimators": 200, "learning_rate": 0.05},
        "description": "GBM lr=0.05",
    },
    {
        "name": "adaboost",
        "params": {"n_estimators": 50, "learning_rate": 1.0},
        "description": "AdaBoost",
    },
    {"name": "bagging", "params": {"n_estimators": 20}, "description": "Bagging"},
]

# –î—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ (3 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)
OTHER_MODELS = [
    {"name": "svr", "params": {"kernel": "rbf", "C": 1.0}, "description": "SVR RBF"},
    {
        "name": "knn",
        "params": {"n_neighbors": 5, "weights": "uniform"},
        "description": "KNN k=5",
    },
    {
        "name": "knn",
        "params": {"n_neighbors": 10, "weights": "distance"},
        "description": "KNN k=10",
    },
]


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


def create_model(model_name: str, params: dict):
    """–°–æ–∑–¥–∞—ë—Ç –º–æ–¥–µ–ª—å –ø–æ –∏–º–µ–Ω–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º."""
    from sklearn.ensemble import (
        AdaBoostRegressor,
        BaggingRegressor,
        ExtraTreesRegressor,
        GradientBoostingRegressor,
        RandomForestRegressor,
    )
    from sklearn.linear_model import (
        ElasticNet,
        HuberRegressor,
        Lasso,
        LinearRegression,
        Ridge,
    )
    from sklearn.neighbors import KNeighborsRegressor
    from sklearn.svm import SVR
    from sklearn.tree import DecisionTreeRegressor

    models = {
        "linear_regression": LinearRegression,
        "ridge": Ridge,
        "lasso": Lasso,
        "elastic_net": ElasticNet,
        "huber": HuberRegressor,
        "decision_tree": DecisionTreeRegressor,
        "random_forest": RandomForestRegressor,
        "extra_trees": ExtraTreesRegressor,
        "gradient_boosting": GradientBoostingRegressor,
        "adaboost": AdaBoostRegressor,
        "bagging": BaggingRegressor,
        "svr": SVR,
        "knn": KNeighborsRegressor,
    }

    model_class = models.get(model_name)
    if model_class is None:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")

    return model_class(**params)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DAG DEFINITION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


@dag(
    dag_id="boston_housing_experiments",
    default_args=default_args,
    description="–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ 19 ML –º–æ–¥–µ–ª–µ–π —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
    schedule=None,  # –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["ml", "boston_housing", "experiments", "parallel"],
    max_active_tasks=8,  # –ú–∞–∫—Å–∏–º—É–º 8 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
)
def boston_housing_experiments_dag():
    """
    DAG –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ ML –º–æ–¥–µ–ª–µ–π
    –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Boston Housing —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # TASK: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @task
    def download_data() -> str:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ Boston Housing."""
        from urllib.request import Request, urlopen

        from loguru import logger

        RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
        output_path = RAW_DATA_DIR / HOUSING_DATA_FILE

        if output_path.exists():
            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç: {output_path}")
            return str(output_path)

        url = "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"
        logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {url}")

        request = Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urlopen(request, timeout=30) as response:
            content = response.read().decode("utf-8")

        lines = content.strip().split("\n")
        if lines and "," in lines[0]:
            data_lines = []
            for line in lines[1:]:
                parts = line.split(",")
                data_lines.append(" ".join(parts))
            content = "\n".join(data_lines)

        with open(output_path, "w") as f:
            f.write(content)

        logger.success(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
        return str(output_path)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # TASK: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @task
    def validate_data(data_path: str) -> dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ train/test split."""
        import pandas as pd
        from loguru import logger
        from sklearn.model_selection import train_test_split

        logger.info(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö: {data_path}")

        df = pd.read_csv(data_path, sep=r"\s+", header=None)
        column_names = [
            "CRIM",
            "ZN",
            "INDUS",
            "CHAS",
            "NOX",
            "RM",
            "AGE",
            "DIS",
            "RAD",
            "TAX",
            "PTRATIO",
            "B",
            "LSTAT",
            "MEDV",
        ]
        df.columns = column_names

        X = df.drop("MEDV", axis=1)
        y = df["MEDV"]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)

        train_data = pd.concat([X_train, y_train], axis=1)
        test_data = pd.concat([X_test, y_test], axis=1)

        train_path = EXPERIMENTS_DIR / "train_data.csv"
        test_path = EXPERIMENTS_DIR / "test_data.csv"

        train_data.to_csv(train_path, index=False)
        test_data.to_csv(test_path, index=False)

        logger.success(
            f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: train={len(X_train)}, test={len(X_test)}"
        )

        return {
            "train_path": str(train_path),
            "test_path": str(test_path),
            "n_train": len(X_train),
            "n_test": len(X_test),
            "features": list(X.columns),
        }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # TASK: –û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è expand)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @task
    def train_single_model(model_config: dict, data_info: dict) -> dict:
        """
        –û–±—É—á–∞–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å expand() –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
        """
        import pickle
        import time

        import numpy as np
        import pandas as pd
        from loguru import logger
        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

        model_name = model_config["name"]
        params = model_config["params"]
        description = model_config["description"]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
        param_str = "_".join([f"{k}={v}" for k, v in params.items()])
        run_id = f"{model_name}_{param_str}" if param_str else model_name

        logger.info(f"üöÄ –û–±—É—á–µ–Ω–∏–µ: {run_id}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        train_df = pd.read_csv(data_info["train_path"])
        test_df = pd.read_csv(data_info["test_path"])

        X_train = train_df.drop("MEDV", axis=1)
        y_train = train_df["MEDV"]
        X_test = test_df.drop("MEDV", axis=1)
        y_test = test_df["MEDV"]

        # –°–æ–∑–¥–∞—ë–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model = create_model(model_name, params)

        start_time = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
        y_pred = model.predict(X_test)

        metrics = {
            "r2_score": float(r2_score(y_test, y_pred)),
            "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
            "mae": float(mean_absolute_error(y_test, y_pred)),
            "mape": float(np.mean(np.abs((y_test - y_pred) / y_test)) * 100),
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        model_path = MODELS_DIR / f"{run_id}.pkl"
        with open(model_path, "wb") as f:
            pickle.dump(model, f)

        logger.success(
            f"‚úÖ {run_id}: R¬≤={metrics['r2_score']:.4f}, RMSE={metrics['rmse']:.4f}"
        )

        return {
            "run_id": run_id,
            "model_name": model_name,
            "description": description,
            "params": params,
            "metrics": metrics,
            "train_time": train_time,
            "model_path": str(model_path),
        }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # TASK: –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @task
    def aggregate_results(
        linear_results: list[dict], tree_results: list[dict], other_results: list[dict]
    ) -> dict:
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."""

        import pandas as pd
        from loguru import logger

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º LazyXComAccess –≤ —Å–ø–∏—Å–∫–∏
        linear_results = list(linear_results) if linear_results else []
        tree_results = list(tree_results) if tree_results else []
        other_results = list(other_results) if other_results else []

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º LazyXComAccess –≤ —Å–ø–∏—Å–∫–∏
        linear_results = list(linear_results) if linear_results else []
        tree_results = list(tree_results) if tree_results else []
        other_results = list(other_results) if other_results else []

        all_results = linear_results + tree_results + other_results

        logger.info(f"üìä –ê–≥—Ä–µ–≥–∞—Ü–∏—è {len(all_results)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")

        # –°–æ–∑–¥–∞—ë–º DataFrame
        rows = []
        for r in all_results:
            rows.append(
                {
                    "run_id": r["run_id"],
                    "model_name": r["model_name"],
                    "description": r["description"],
                    "r2_score": r["metrics"]["r2_score"],
                    "rmse": r["metrics"]["rmse"],
                    "mae": r["metrics"]["mae"],
                    "mape": r["metrics"]["mape"],
                    "train_time": r["train_time"],
                    "model_path": r["model_path"],
                }
            )

        df = pd.DataFrame(rows)
        df = df.sort_values("r2_score", ascending=False)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_path = EXPERIMENTS_DIR / "all_results.csv"
        df.to_csv(results_path, index=False)

        # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º
        best_overall = df.iloc[0].to_dict()

        logger.success(
            f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_overall['run_id']} (R¬≤={best_overall['r2_score']:.4f})"
        )

        return {
            "total_experiments": len(all_results),
            "results_path": str(results_path),
            "best_model": best_overall,
            "summary": {
                "mean_r2": float(df["r2_score"].mean()),
                "std_r2": float(df["r2_score"].std()),
                "best_r2": float(df["r2_score"].max()),
                "worst_r2": float(df["r2_score"].min()),
            },
        }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # TASK: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @task
    def generate_report(aggregated: dict) -> dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ MinIO."""
        from datetime import datetime

        import boto3
        import pandas as pd
        from botocore.client import Config
        from loguru import logger

        logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # –ß–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        df = pd.read_csv(aggregated["results_path"])

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º markdown –æ—Ç—á—ë—Ç
        report = f"""# Boston Housing ML Experiments Report
## –î–∞—Ç–∞: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

### –°–≤–æ–¥–∫–∞
- –í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {aggregated["total_experiments"]}
- –õ—É—á—à–∏–π R¬≤: {aggregated["summary"]["best_r2"]:.4f}
- –°—Ä–µ–¥–Ω–∏–π R¬≤: {aggregated["summary"]["mean_r2"]:.4f} ¬± {aggregated["summary"]["std_r2"]:.4f}

### –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
- **–ú–æ–¥–µ–ª—å:** {aggregated["best_model"]["run_id"]}
- **R¬≤ Score:** {aggregated["best_model"]["r2_score"]:.4f}
- **RMSE:** {aggregated["best_model"]["rmse"]:.4f}
- **MAE:** {aggregated["best_model"]["mae"]:.4f}

### –¢–æ–ø-5 –º–æ–¥–µ–ª–µ–π

| –†–∞–Ω–≥ | –ú–æ–¥–µ–ª—å | R¬≤ | RMSE | MAE |
|------|--------|-----|------|-----|
"""
        for i, row in df.head(5).iterrows():
            report += f"| {i + 1} | {row['run_id'][:30]} | {row['r2_score']:.4f} | {row['rmse']:.4f} | {row['mae']:.4f} |\n"

        report += """
### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

#### –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏
"""
        linear_df = df[
            df["model_name"].isin(
                ["linear_regression", "ridge", "lasso", "elastic_net", "huber"]
            )
        ]
        if not linear_df.empty:
            best_linear = linear_df.iloc[0]
            report += f"- –õ—É—á—à–∞—è: {best_linear['run_id']} (R¬≤={best_linear['r2_score']:.4f})\n"

        report += """
#### –î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏
"""
        tree_names = [
            "decision_tree",
            "random_forest",
            "extra_trees",
            "gradient_boosting",
            "adaboost",
            "bagging",
        ]
        tree_df = df[df["model_name"].isin(tree_names)]
        if not tree_df.empty:
            best_tree = tree_df.iloc[0]
            report += (
                f"- –õ—É—á—à–∞—è: {best_tree['run_id']} (R¬≤={best_tree['r2_score']:.4f})\n"
            )

        report += """
#### –î—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏
"""
        other_df = df[df["model_name"].isin(["svr", "knn"])]
        if not other_df.empty:
            best_other = other_df.iloc[0]
            report += (
                f"- –õ—É—á—à–∞—è: {best_other['run_id']} (R¬≤={best_other['r2_score']:.4f})\n"
            )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
        report_path = EXPERIMENTS_DIR / f"report_{timestamp}.md"
        with open(report_path, "w") as f:
            f.write(report)

        logger.success(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_path}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ MinIO
        try:
            minio_endpoint = os.environ.get(
                "MLFLOW_S3_ENDPOINT_URL", "http://minio:9000"
            )
            aws_access_key = os.environ.get("AWS_ACCESS_KEY_ID", "minioadmin")
            aws_secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "minioadmin")

            s3_client = boto3.client(
                "s3",
                endpoint_url=minio_endpoint,
                aws_access_key_id=aws_access_key,
                aws_secret_access_key=aws_secret_key,
                config=Config(signature_version="s3v4"),
            )

            bucket_name = "airflow-artifacts"

            try:
                s3_client.head_bucket(Bucket=bucket_name)
            except Exception:
                s3_client.create_bucket(Bucket=bucket_name)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç—á—ë—Ç
            s3_client.upload_file(
                str(report_path), bucket_name, f"reports/report_{timestamp}.md"
            )

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            s3_client.upload_file(
                aggregated["results_path"],
                bucket_name,
                f"results/all_results_{timestamp}.csv",
            )

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            s3_client.upload_file(
                aggregated["best_model"]["model_path"],
                bucket_name,
                f"models/best_model_{timestamp}.pkl",
            )

            logger.success("‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ MinIO")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ MinIO: {e}")

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
        try:
            import mlflow

            mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000")
            mlflow.set_tracking_uri(mlflow_uri)
            mlflow.set_experiment("boston_housing_experiments")

            with mlflow.start_run(run_name=f"experiments_summary_{timestamp}"):
                mlflow.log_metric("total_experiments", aggregated["total_experiments"])
                mlflow.log_metric("best_r2", aggregated["summary"]["best_r2"])
                mlflow.log_metric("mean_r2", aggregated["summary"]["mean_r2"])
                mlflow.log_param("best_model", aggregated["best_model"]["run_id"])
                mlflow.log_artifact(str(report_path), "reports")
                mlflow.log_artifact(aggregated["results_path"], "results")
                mlflow.set_tag("source", "airflow")
                mlflow.set_tag("dag", "boston_housing_experiments")

            logger.success("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ MLflow")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ MLflow: {e}")

        return {
            "status": "success",
            "report_path": str(report_path),
            "total_experiments": aggregated["total_experiments"],
            "best_model": aggregated["best_model"]["run_id"],
            "best_r2": aggregated["summary"]["best_r2"],
        }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô –ò –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û–ì–û –í–´–ü–û–õ–ù–ï–ù–ò–Ø
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data_path = download_data()

    # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data_info = validate_data(data_path)

    # 3. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º expand()

    # –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏
    linear_results = train_single_model.expand(
        model_config=LINEAR_MODELS,
        data_info=[data_info] * len(LINEAR_MODELS),
    )

    # –î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏
    tree_results = train_single_model.expand(
        model_config=TREE_MODELS,
        data_info=[data_info] * len(TREE_MODELS),
    )

    # –î—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏
    other_results = train_single_model.expand(
        model_config=OTHER_MODELS,
        data_info=[data_info] * len(OTHER_MODELS),
    )

    # 4. –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∂–¥—ë—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π)
    aggregated = aggregate_results(
        linear_results=linear_results,
        tree_results=tree_results,
        other_results=other_results,
    )

    # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    generate_report(aggregated)


# –°–æ–∑–¥–∞–Ω–∏–µ DAG
boston_housing_experiments_dag()
