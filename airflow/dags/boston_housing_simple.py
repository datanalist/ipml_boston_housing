"""
DAG: Boston Housing Simple Pipeline
===================================
ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Random Forest.

Ğ­Ñ‚Ğ°Ğ¿Ñ‹:
1. download_data - Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
2. validate_data - Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
3. train_model - ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
4. evaluate_model - ĞÑ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
5. save_to_minio - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² MinIO
"""

import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

from airflow.decorators import dag, task

# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿ÑƒÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
sys.path.insert(0, "/opt/airflow")
sys.path.insert(0, "/opt/airflow/src")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ DAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

default_args = {
    "owner": "boston_housing",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

# ĞŸÑƒÑ‚Ğ¸ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼
DATA_DIR = Path("/opt/airflow/data")
RAW_DATA_DIR = DATA_DIR / "raw"
MODELS_DIR = DATA_DIR / "models"
HOUSING_DATA_FILE = "housing.csv"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DAG DEFINITION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@dag(
    dag_id="boston_housing_simple",
    default_args=default_args,
    description="ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ ML Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½: Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° â†’ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ â†’ Ğ¾Ñ†ĞµĞ½ĞºĞ°",
    schedule=None,  # Ğ ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["ml", "boston_housing", "random_forest"],
)
def boston_housing_simple_dag():
    """
    ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Random Forest
    Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Boston Housing.
    """

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK 1: Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task
    def download_data() -> str:
        """
        Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Boston Housing Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°.
        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….
        """
        from urllib.request import Request, urlopen
        from urllib.error import HTTPError, URLError

        from loguru import logger

        RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
        output_path = RAW_DATA_DIR / HOUSING_DATA_FILE

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»Ğ¸ Ñ„Ğ°Ğ¹Ğ» (ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ)
        if output_path.exists():
            logger.info(f"âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚: {output_path}")
            return str(output_path)

        # URL Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        url = "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"

        logger.info(f"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· {url}")

        try:
            request = Request(url, headers={"User-Agent": "Mozilla/5.0"})
            with urlopen(request, timeout=30) as response:
                content = response.read().decode("utf-8")

            # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ CSV Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ĞµĞ¼
            lines = content.strip().split("\n")
            if lines and "," in lines[0]:
                data_lines = []
                for line in lines[1:]:  # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
                    parts = line.split(",")
                    data_lines.append(" ".join(parts))
                content = "\n".join(data_lines)

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            with open(output_path, "w") as f:
                f.write(content)

            logger.success(f"âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {output_path}")
            return str(output_path)

        except (URLError, HTTPError) as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
            raise

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK 2: Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task
    def validate_data(data_path: str) -> dict:
        """
        ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….
        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼.
        """
        import pandas as pd
        from loguru import logger

        logger.info(f"ğŸ” Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {data_path}")

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        df = pd.read_csv(data_path, sep=r"\s+", header=None)

        # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
        column_names = [
            "CRIM",
            "ZN",
            "INDUS",
            "CHAS",
            "NOX",
            "RM",
            "AGE",
            "DIS",
            "RAD",
            "TAX",
            "PTRATIO",
            "B",
            "LSTAT",
            "MEDV",
        ]
        df.columns = column_names

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸
        stats = {
            "n_samples": len(df),
            "n_features": len(df.columns) - 1,
            "missing_values": int(df.isnull().sum().sum()),
            "target_mean": float(df["MEDV"].mean()),
            "target_std": float(df["MEDV"].std()),
        }

        # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
        assert stats["n_samples"] > 0, "Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿ÑƒÑÑ‚!"
        assert stats["missing_values"] == 0, "Ğ•ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ!"
        assert stats["n_features"] == 13, (
            f"ĞĞ¶Ğ¸Ğ´Ğ°Ğ»Ğ¾ÑÑŒ 13 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¾ {stats['n_features']}"
        )

        logger.success(f"âœ… Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ğ°: {stats['n_samples']} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
        logger.info(
            f"   Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ MEDV: {stats['target_mean']:.2f} Â± {stats['target_std']:.2f}"
        )

        return stats

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK 3: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task
    def train_model(data_path: str, data_stats: dict) -> dict:
        """
        ĞĞ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Random Forest.
        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹.
        """
        import pickle

        import pandas as pd
        from loguru import logger
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.model_selection import train_test_split

        logger.info("ğŸš€ ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Random Forest")

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        df = pd.read_csv(data_path, sep=r"\s+", header=None)
        column_names = [
            "CRIM",
            "ZN",
            "INDUS",
            "CHAS",
            "NOX",
            "RM",
            "AGE",
            "DIS",
            "RAD",
            "TAX",
            "PTRATIO",
            "B",
            "LSTAT",
            "MEDV",
        ]
        df.columns = column_names

        X = df.drop("MEDV", axis=1)
        y = df["MEDV"]

        # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        params = {
            "n_estimators": 100,
            "max_depth": 10,
            "min_samples_split": 5,
            "min_samples_leaf": 2,
            "random_state": 42,
        }

        # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
        model = RandomForestRegressor(**params, n_jobs=-1)
        model.fit(X_train, y_train)

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        model_path = MODELS_DIR / "random_forest_airflow.pkl"

        with open(model_path, "wb") as f:
            pickle.dump(model, f)

        logger.success(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°: {model_path}")

        # Ğ’Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
        feature_importance = dict(zip(X.columns, model.feature_importances_))
        top_features = sorted(
            feature_importance.items(), key=lambda x: x[1], reverse=True
        )[:5]

        logger.info("ğŸ“Š Ğ¢Ğ¾Ğ¿-5 Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²:")
        for feat, imp in top_features:
            logger.info(f"   {feat}: {imp:.4f}")

        return {
            "model_path": str(model_path),
            "params": params,
            "train_size": len(X_train),
            "test_size": len(X_test),
            "feature_columns": list(X.columns),
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK 4: ĞÑ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task
    def evaluate_model(data_path: str, train_result: dict) -> dict:
        """
        ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….
        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.
        """
        import pickle

        import numpy as np
        import pandas as pd
        from loguru import logger
        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
        from sklearn.model_selection import train_test_split

        logger.info("ğŸ“ˆ ĞÑ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        with open(train_result["model_path"], "rb") as f:
            model = pickle.load(f)

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        df = pd.read_csv(data_path, sep=r"\s+", header=None)
        column_names = [
            "CRIM",
            "ZN",
            "INDUS",
            "CHAS",
            "NOX",
            "RM",
            "AGE",
            "DIS",
            "RAD",
            "TAX",
            "PTRATIO",
            "B",
            "LSTAT",
            "MEDV",
        ]
        df.columns = column_names

        X = df.drop("MEDV", axis=1)
        y = df["MEDV"]

        # Ğ’Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ñ‚Ğ¾ Ğ¶Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ
        _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
        y_pred = model.predict(X_test)

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        metrics = {
            "r2_score": float(r2_score(y_test, y_pred)),
            "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
            "mae": float(mean_absolute_error(y_test, y_pred)),
            "mape": float(np.mean(np.abs((y_test - y_pred) / y_test)) * 100),
        }

        logger.success("âœ… ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:")
        logger.info(f"   RÂ² Score:  {metrics['r2_score']:.4f}")
        logger.info(f"   RMSE:      {metrics['rmse']:.4f}")
        logger.info(f"   MAE:       {metrics['mae']:.4f}")
        logger.info(f"   MAPE:      {metrics['mape']:.2f}%")

        return {
            "model_path": train_result["model_path"],
            "metrics": metrics,
            "params": train_result["params"],
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # TASK 5: Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² MinIO Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² MLflow
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @task
    def save_artifacts(evaluation_result: dict) -> dict:
        """
        Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² MinIO Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² MLflow.
        """
        import json
        from datetime import datetime

        import boto3
        from botocore.client import Config
        from loguru import logger

        logger.info("ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²")

        # ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ MinIO
        minio_endpoint = os.environ.get("MLFLOW_S3_ENDPOINT_URL", "http://minio:9000")
        aws_access_key = os.environ.get("AWS_ACCESS_KEY_ID", "minioadmin")
        aws_secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "minioadmin")

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ĞºĞ»Ğ¸ĞµĞ½Ñ‚ S3
        s3_client = boto3.client(
            "s3",
            endpoint_url=minio_endpoint,
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            config=Config(signature_version="s3v4"),
        )

        # Ğ˜Ğ¼Ñ Ğ±Ğ°ĞºĞµÑ‚Ğ°
        bucket_name = "airflow-artifacts"

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ±Ğ°ĞºĞµÑ‚ ĞµÑĞ»Ğ¸ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
        try:
            s3_client.head_bucket(Bucket=bucket_name)
        except Exception:
            s3_client.create_bucket(Bucket=bucket_name)
            logger.info(f"ğŸ“¦ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ Ğ±Ğ°ĞºĞµÑ‚: {bucket_name}")

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² MinIO
        model_path = evaluation_result["model_path"]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        s3_key = f"models/random_forest_{timestamp}.pkl"

        s3_client.upload_file(model_path, bucket_name, s3_key)
        logger.success(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ² MinIO: s3://{bucket_name}/{s3_key}")

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ğº JSON
        metrics_key = f"metrics/random_forest_{timestamp}.json"
        metrics_json = json.dumps(
            {
                "metrics": evaluation_result["metrics"],
                "params": evaluation_result["params"],
                "timestamp": timestamp,
            },
            indent=2,
        )

        s3_client.put_object(
            Bucket=bucket_name,
            Key=metrics_key,
            Body=metrics_json.encode("utf-8"),
            ContentType="application/json",
        )
        logger.success(f"âœ… ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: s3://{bucket_name}/{metrics_key}")

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² MLflow (ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½)
        try:
            import mlflow

            mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000")
            mlflow.set_tracking_uri(mlflow_uri)
            mlflow.set_experiment("boston_housing_airflow")

            with mlflow.start_run(run_name=f"airflow_run_{timestamp}"):
                # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
                for param_name, param_value in evaluation_result["params"].items():
                    mlflow.log_param(param_name, param_value)

                # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
                for metric_name, metric_value in evaluation_result["metrics"].items():
                    mlflow.log_metric(metric_name, metric_value)

                # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                mlflow.log_artifact(model_path, "model")

                mlflow.set_tag("source", "airflow")
                mlflow.set_tag("dag", "boston_housing_simple")

            logger.success("âœ… Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ² MLflow")

        except Exception as e:
            logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² MLflow: {e}")

        return {
            "status": "success",
            "minio_model_path": f"s3://{bucket_name}/{s3_key}",
            "minio_metrics_path": f"s3://{bucket_name}/{metrics_key}",
            "metrics": evaluation_result["metrics"],
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ•Ğ™
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    data_path = download_data()

    # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    data_stats = validate_data(data_path)

    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸)
    train_result = train_model(data_path, data_stats)

    # ĞÑ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    eval_result = evaluate_model(data_path, train_result)

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²
    save_artifacts(eval_result)


# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ DAG
boston_housing_simple_dag()
